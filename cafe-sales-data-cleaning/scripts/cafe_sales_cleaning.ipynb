{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496e6183",
   "metadata": {},
   "source": [
    "# Data Cleaning and Analysis\n",
    "\n",
    "**Workflow:** Data Cleaning ‚Üí QA ‚Üí EDA \n",
    "\n",
    "**Goal:** Clean and prepare raw cafe POS data, then explore sales patterns for actionable insights.\n",
    "\n",
    "**Data:** Raw data with missing values, duplicates, and inconsistent categories.\n",
    "\n",
    "**Outputs:**\n",
    "- `datasets/cleansed_cafe_sales.csv` (cleaned dataset for downstream analysis)\n",
    "- `results/monthly_sales.csv`, `results/top_products.csv`, `results/customer_summary.csv`\n",
    "\n",
    "**Run Order:** Top ‚Üí Bottom  \n",
    "**Environment:** Python 3.10+, pandas, numpy, matplotlib\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning Steps\n",
    "1) Setup & Dependencies  \n",
    "2) Raw Data Loading\n",
    "3) Data Inspection (shape, schema, missing values)  \n",
    "4) Global Data Cleaning  \n",
    "5) Data Type Standardization (dates, numeric fields, categoricals)  \n",
    "6) Business-rule repairs & quality checks  \n",
    "7) Final Validation & Export  ‚Üí `datasets/cleansed_cafe_sales.csv`\n",
    "\n",
    "## EDA & Analysis Steps\n",
    "8) Monthly sales trend and seasonality  ‚Üí export `results/monthly_sales.csv`  \n",
    "9) Top products by sales (Top-N)       ‚Üí export `results/top_products.csv`  \n",
    "10) Customer summary (orders, AOV, pareto) ‚Üí export `results/customer_summary.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75836f1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies  \n",
    "*Purpose:* Import core libraries, define relative paths, and set display options.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Project Paths\n",
    "# Use relative paths so the notebook works across different machines.\n",
    "# Raw datasets are stored in `datasets/`, analysis outputs in `results/`.\n",
    "# -------------------------------------------------------------------\n",
    "DATA_DIR = Path(\"datasets\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# File Definitions\n",
    "# Input:  raw_cafe_sales.csv   (uncleaned POS transactions)\n",
    "# Output: cleansed_cafe_sales.csv (cleaned dataset for downstream analysis)\n",
    "# -------------------------------------------------------------------\n",
    "RAW_FILE = DATA_DIR / \"raw_cafe_sales.csv\"\n",
    "CLEANSED_FILE = DATA_DIR / \"cleansed_cafe_sales.csv\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Display Settings\n",
    "# Configure pandas to show floats with comma separators and 2 decimals.\n",
    "# Example: 123456.789 -> 123,456.79\n",
    "# -------------------------------------------------------------------\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317094f",
   "metadata": {},
   "source": [
    "## 2: Raw Data Loading\n",
    "*Purpose:* Read source CSV and set basic options (encoding, dtypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef9c7b36d4999b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.469062Z",
     "start_time": "2025-05-19T00:13:32.458820Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686840287d5cd0a",
   "metadata": {},
   "source": [
    "## 3: Data Inspection\n",
    "*Purpose:* Understand dataset size, schema, and missing values before applying cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1043c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_4977031</td>\n",
       "      <td>Cake</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_4271903</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_7034554</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_3160411</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Transaction ID    Item Quantity Price Per Unit Total Spent  Payment Method  \\\n",
       "0    TXN_1961373  Coffee        2            2.0         4.0     Credit Card   \n",
       "1    TXN_4977031    Cake        4            3.0        12.0            Cash   \n",
       "2    TXN_4271903  Cookie        4            1.0       ERROR     Credit Card   \n",
       "3    TXN_7034554   Salad        2            5.0        10.0         UNKNOWN   \n",
       "4    TXN_3160411  Coffee        2            2.0         4.0  Digital Wallet   \n",
       "\n",
       "   Location Transaction Date  \n",
       "0  Takeaway       2023-09-08  \n",
       "1  In-store       2023-05-16  \n",
       "2  In-store       2023-07-19  \n",
       "3   UNKNOWN       2023-04-27  \n",
       "4  In-store       2023-06-11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Transaction ID      object\n",
       "Item                object\n",
       "Quantity            object\n",
       "Price Per Unit      object\n",
       "Total Spent         object\n",
       "Payment Method      object\n",
       "Location            object\n",
       "Transaction Date    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Location            3265\n",
       "Payment Method      2579\n",
       "Item                 333\n",
       "Price Per Unit       179\n",
       "Total Spent          173\n",
       "Transaction Date     159\n",
       "Quantity             138\n",
       "Transaction ID         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(5))\n",
    "display(df.dtypes)\n",
    "\n",
    "# Basic nulls & duplicates\n",
    "display(df.isna().sum().sort_values(ascending=False))\n",
    "dup_cnt = df.duplicated(subset=[\"transaction_id\"]).sum() if \"transaction_id\" in df.columns else df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d42e4c",
   "metadata": {},
   "source": [
    "‚úÖ Inspection results:\n",
    "- Shape: 10000 rows √ó 8 columns  \n",
    "- Columns: `Transaction ID`, `Item`, `Quantity`, `Price Per Unit`, `Total Spent`, `Payment Method`, `Location`, `Transaction Date`\n",
    "- Notable missing values:\n",
    "    - Location            3265\n",
    "    - Payment Method      2579\n",
    "    - Item                 333\n",
    "    - Price Per Unit       179\n",
    "    - Total Spent          173\n",
    "    - Transaction Date     159\n",
    "    - Quantity             138\n",
    "- No duplicate rows\n",
    "- The schema includes **numeric**, **categorical**, and **datetime** fields, but they are all represented as **objects**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e089f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Dictionary & Metric Definitions  \n",
    "\n",
    "**Core fields**\n",
    "- `transaction_id` (str): unique ID per transaction\n",
    "- `order_datetime` (datetime): local timestamp of the transaction\n",
    "- `product` (str): product name\n",
    "- `category` (str): normalized category name\n",
    "- `unit_price` (float): price per unit (>= 0)\n",
    "- `quantity` (int): number of units (> 0)\n",
    "- `payment_method` (str): e.g., cash/card (optional)\n",
    "\n",
    "**Key Assumptions / Rules**\n",
    "- `transaction_id` must be unique  \n",
    "- `order_datetime` cannot be in the future  \n",
    "- `unit_price >= 0`, `quantity > 0`, `sales_amount >= 0`  \n",
    "- Category spelling standardized \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f0278",
   "metadata": {},
   "source": [
    "## 4: Global Data Cleaning\n",
    "*Purpose:* Remove duplicates, standardize placeholder values, and assess missing data globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28a4f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed: 0\n",
      "Placeholder values ('ERROR', 'UNKNOWN') standardized to NaN\n",
      "\n",
      "Missing values after global cleaning:\n",
      "Location            3961\n",
      "Payment Method      3178\n",
      "Item                 969\n",
      "Price Per Unit       533\n",
      "Total Spent          502\n",
      "Quantity             479\n",
      "Transaction Date     460\n",
      "Transaction ID         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Remove duplicates if any\n",
    "initial_rows = len(df)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "duplicates_removed = initial_rows - len(df)\n",
    "print(f\"Duplicate rows removed: {duplicates_removed}\")\n",
    "\n",
    "# 4.2 Standardize placeholder values to NaN\n",
    "df = df.replace(['ERROR', 'UNKNOWN'], np.nan) \n",
    "print(\"Placeholder values ('ERROR', 'UNKNOWN') standardized to NaN\")\n",
    "\n",
    "# 4.3 Reassess missing values globally\n",
    "print(f\"\\nMissing values after global cleaning:\") \n",
    "print(df.isna().sum().sort_values(ascending=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aae171",
   "metadata": {},
   "source": [
    "‚úÖ Cleaning done:\n",
    "- Removed duplicates (if any).  \n",
    "- Standardized placeholder values ('ERROR', 'UNKNOWN') as NaN.  \n",
    "- Missing value counts **increased** because invalid entries were converted to NaN ‚Äî this is expected and ensures data integrity.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3cec57b5fb06a",
   "metadata": {},
   "source": [
    "## 5: Data Type Standardization\n",
    "*Purpose:* Convert columns to appropriate data types for reliable calculations and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce4ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace cleaned in: ['Item', 'Payment Method', 'Location']\n",
      "Numeric columns converted: Quantity, Price Per Unit, Total Spent\n",
      "DateTime column converted: Transaction Date\n",
      "\n",
      "Data types after standardization:\n",
      "Transaction ID              object\n",
      "Item                        object\n",
      "Quantity                   float64\n",
      "Price Per Unit             float64\n",
      "Total Spent                float64\n",
      "Payment Method              object\n",
      "Location                    object\n",
      "Transaction Date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Missing values after type conversion:\n",
      "Location            3961\n",
      "Payment Method      3178\n",
      "Item                 969\n",
      "Price Per Unit       533\n",
      "Total Spent          502\n",
      "Quantity             479\n",
      "Transaction Date     460\n",
      "Transaction ID         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Strip whitespace in categorical columns\n",
    "categorical_cols = ['Item', 'Payment Method', 'Location']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "print(f\"Whitespace cleaned in: {categorical_cols}\")\n",
    "\n",
    "# 5.2 Convert data types with error coercion\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce') \n",
    "df[['Price Per Unit', 'Total Spent']] = df[['Price Per Unit', 'Total Spent']].apply(pd.to_numeric, errors='coerce') \n",
    "print(\"Numeric columns converted: Quantity, Price Per Unit, Total Spent\")\n",
    "\n",
    "# 5.3 Parse dates with error coercion\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce') \n",
    "print(\"DateTime column converted: Transaction Date\")\n",
    "\n",
    "# 5.4 Final reassessment of data types and missing values\n",
    "print(f\"\\nData types after standardization:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values after type conversion:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e941587",
   "metadata": {},
   "source": [
    "‚úÖ Data types standardized:\n",
    "- Stripped whitespace in categorical fields (`Item`, `Payment Method`, `Location`).  \n",
    "- `Quantity` converted to **integer**, `Price Per Unit` and `Total Spent` to **float**.  \n",
    "- `Transaction Date` parsed as **datetime**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098446300415e01",
   "metadata": {},
   "source": [
    "## 6: Business-rule repairs & quality checks\n",
    "*Purpose:* Column-by-column fixes based on simple domain rules: review categories, impute missing values from empirical distributions, and infer `Item` from unit price when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e176a",
   "metadata": {},
   "source": [
    "### 6.1.1: Data Quality Validation\n",
    "Verify core business rules and data integrity before detailed cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9aeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Transaction ID Uniqueness:\n",
      " Duplicate Transaction IDs: 0\n",
      " ‚úÖ All Transaction IDs are unique\n",
      "\n",
      "2. Date Range Validation:\n",
      " Future dates: 0\n",
      " ‚úÖ No future dates\n",
      "\n",
      "3. Numeric Range Validation & Cleanup:\n",
      " Quantity <= 0 or NaN: 479\n",
      " Price Per Unit < 0 or NaN: 533\n",
      " Total Spent < 0 or NaN: 502\n",
      " ‚ö†Ô∏è WARNING: Found 1456 rows with invalid data!\n",
      " üîß FIXED: Removed all invalid rows\n",
      "\n",
      "Rows remaining after cleanup: 8544\n",
      "Data quality: 85.4% of original data retained\n",
      "=== VALIDATION & CLEANUP COMPLETE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Transaction ID Uniqueness\n",
    "# =========================================================\n",
    "print(\"1. Transaction ID Uniqueness:\") \n",
    "duplicate_ids = df['Transaction ID'].duplicated().sum() \n",
    "\n",
    "print(f\" Duplicate Transaction IDs: {duplicate_ids}\") \n",
    "if duplicate_ids > 0: \n",
    "    print(\" ‚ö†Ô∏è WARNING: Non-unique transaction IDs found!\") \n",
    "    # Remove duplicates based on Transaction ID\n",
    "    df = df.drop_duplicates(subset=['Transaction ID'], keep='first')\n",
    "    print(\" üîß FIXED: Removed duplicate Transaction IDs\")\n",
    "else: \n",
    "    print(\" ‚úÖ All Transaction IDs are unique\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. Date Range Validation (no future dates)\n",
    "# =========================================================\n",
    "print(\"\\n2. Date Range Validation:\")\n",
    "df_temp_date = pd.to_datetime(df['Transaction Date'], errors='coerce')\n",
    "today = pd.Timestamp.now().normalize()\n",
    "future_dates = (df_temp_date > today).sum()\n",
    "print(f\" Future dates: {future_dates}\")\n",
    "if future_dates > 0:\n",
    "    print(\" ‚ö†Ô∏è WARNING: Future dates found!\")\n",
    "    # Remove rows with future dates\n",
    "    df = df[df_temp_date <= today].reset_index(drop=True)\n",
    "    print(\" üîß FIXED: Removed rows with future dates\")\n",
    "else:\n",
    "    print(\" ‚úÖ No future dates\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. Numeric Range Validation & Cleanup\n",
    "# =========================================================\n",
    "print(\"\\n3. Numeric Range Validation & Cleanup:\")\n",
    "\n",
    "# --- Step 3.1: Record initial row count ---\n",
    "initial_count = len(df)\n",
    "\n",
    "# --- Step 3.2: Convert to numeric (force invalid to NaN) ---\n",
    "quantity_numeric = pd.to_numeric(df['Quantity'], errors='coerce') \n",
    "price_numeric = pd.to_numeric(df['Price Per Unit'], errors='coerce') \n",
    "total_numeric = pd.to_numeric(df['Total Spent'], errors='coerce')\n",
    "\n",
    "# --- Step 3.3: Identify invalid values ---\n",
    "invalid_qty_mask = (quantity_numeric <= 0) | quantity_numeric.isna()\n",
    "invalid_price_mask = (price_numeric < 0) | price_numeric.isna()\n",
    "invalid_total_mask = (total_numeric < 0) | total_numeric.isna()\n",
    "\n",
    "print(f\" Quantity <= 0 or NaN: {invalid_qty_mask.sum()}\")\n",
    "print(f\" Price Per Unit < 0 or NaN: {invalid_price_mask.sum()}\")\n",
    "print(f\" Total Spent < 0 or NaN: {invalid_total_mask.sum()}\")\n",
    "\n",
    "# --- Step 3.4: Remove invalid rows ---\n",
    "valid_data_mask = ~(invalid_qty_mask | invalid_price_mask | invalid_total_mask)\n",
    "df_clean = df[valid_data_mask].reset_index(drop=True)\n",
    "\n",
    "removed_count = initial_count - len(df_clean)\n",
    "if removed_count > 0:\n",
    "    print(f\" ‚ö†Ô∏è WARNING: Found {removed_count} rows with invalid data!\")\n",
    "    print(f\" üîß FIXED: Removed all invalid rows\")\n",
    "    df = df_clean  # Update the main dataframe\n",
    "else:\n",
    "    print(\" ‚úÖ All numeric values are valid\")\n",
    "\n",
    "# --- Step 3.5: Report final status ---\n",
    "print(f\"\\nRows remaining after cleanup: {len(df)}\")\n",
    "print(f\"Data quality: {len(df)/initial_count*100:.1f}% of original data retained\")\n",
    "print(\"=== VALIDATION & CLEANUP COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b877b62",
   "metadata": {},
   "source": [
    "### 6.1.2: Typo checks for **categorical** columns\n",
    "Quick review on categorical columns (`Item`,`Payment Method`, and `Location`) to catch spelling/case/whitespace issues before any imputation or inference.  \n",
    "**Review only**. No modification here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b18b851495941ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.781091Z",
     "start_time": "2025-05-19T00:13:32.777887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Juice       1011\n",
       "Coffee      1004\n",
       "Cake         993\n",
       "Salad        981\n",
       "Sandwich     964\n",
       "Tea          928\n",
       "Smoothie     927\n",
       "Cookie       924\n",
       "Name: Item, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 'Item', 'Payment Method', and 'Location' for misspelling\n",
    "df['Item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8867d6ca67762a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.827755Z",
     "start_time": "2025-05-19T00:13:32.824398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Digital Wallet    1973\n",
       "Credit Card       1952\n",
       "Cash              1928\n",
       "Name: Payment Method, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Payment Method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96623cf919a56cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.869214Z",
     "start_time": "2025-05-19T00:13:32.866339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In-store    2597\n",
       "Takeaway    2574\n",
       "Name: Location, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b2e5bec412bf0",
   "metadata": {},
   "source": [
    "‚úÖ Observation: No obvious typos found in `Item`, `Payment Method` and `Location`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8719914",
   "metadata": {},
   "source": [
    "### 6.1.3 Category Standardization\n",
    "Apply consistent spelling and formatting to categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e28ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Category standardization completed:\n",
      "Items after standardization:\n",
      "Juice       1011\n",
      "Coffee      1004\n",
      "Cake         993\n",
      "Salad        981\n",
      "Sandwich     964\n",
      "Tea          928\n",
      "Smoothie     927\n",
      "Cookie       924\n",
      "Name: Item, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1 Category Standardization\n",
    "# Apply consistent spelling and formatting to categorical values.\n",
    "item_standardization = {'Juice': 'Juice', 'coffee': 'Coffee','salad': 'Salad', 'cake': 'Cake', 'sandwich': 'Sandwich', 'smoothie': 'Smoothie', 'cookie': 'Cookie', 'tea': 'Tea'}\n",
    "\n",
    "# 2 Apply standardization to Items\n",
    "# Capitalize first letter of each word in 'Item'\n",
    "df['Item'] = df['Item'].str.title()\n",
    "\n",
    "# 3 Standardize item names based on mapping\n",
    "for old_name, new_name in item_standardization.items():\n",
    "    df['Item'] = df['Item'].str.replace(old_name, new_name, case=False)\n",
    "\n",
    "# 4 Standardize Payment Method\n",
    "df['Payment Method'] = df['Payment Method'].str.title()\n",
    "\n",
    "# 5 Standardize Location\n",
    "df['Location'] = df['Location'].str.title()\n",
    "\n",
    "print(\"‚úÖ Category standardization completed:\") \n",
    "print(\"Items after standardization:\") \n",
    "print(df['Item'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3588a6",
   "metadata": {},
   "source": [
    "### 6.2 Filling `Item` ‚Äî infer from `Price Per Unit`\n",
    "If unit price uniquely identifies an item, fill missing `Item` using a price ‚Üí item mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09f031ab88baec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.898304Z",
     "start_time": "2025-05-19T00:13:32.894267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item\n",
       "Cake        [3.0]\n",
       "Coffee      [2.0]\n",
       "Cookie      [1.0]\n",
       "Juice       [3.0]\n",
       "Salad       [5.0]\n",
       "Sandwich    [4.0]\n",
       "Smoothie    [4.0]\n",
       "Tea         [1.5]\n",
       "Name: Price Per Unit, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check price spread per item to catch mislabeled entries \n",
    "# and for next step of inferring missing 'Item'\n",
    "df.groupby('Item')['Price Per Unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14934cdd7e11f112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:32.943156Z",
     "start_time": "2025-05-19T00:13:32.941450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coffee      1102\n",
       "Salad       1094\n",
       "Cookie      1030\n",
       "Tea         1022\n",
       "Juice       1011\n",
       "Cake         993\n",
       "Sandwich     964\n",
       "Smoothie     927\n",
       "Name: Item, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping from unique prices to corresponding products\n",
    "price_to_item = {\n",
    "    1.0: 'Cookie',\n",
    "    1.5: 'Tea',\n",
    "    2.0: 'Coffee',\n",
    "    5.0: 'Salad'\n",
    "}\n",
    "\n",
    "# fill in the 'Item' value\n",
    "mask = df['Item'].isna() & df['Price Per Unit'].isin(price_to_item.keys())\n",
    "df.loc[mask, 'Item'] = df.loc[mask, 'Price Per Unit'].map(price_to_item)\n",
    "\n",
    "# check how much been filled\n",
    "df['Item'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb8cae0ccefcd7",
   "metadata": {},
   "source": [
    "‚úÖ `Item` column filled where unit price provided a unique mapping. Remaining nulls will be filled in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa2ea6b26cff36",
   "metadata": {},
   "source": [
    "### 6.3 Filling `Quantity`, `Price Per Unit`, and `Total Spent`\n",
    "The counts of missing values in these three columns differ.  \n",
    "When two values are present, the third can be derived to complete the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d41ac667bd0cdcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.039533Z",
     "start_time": "2025-05-19T00:13:33.003919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill Total Spent = Quantity * Price Per Unit\n",
    "mask_ts = df['Total Spent'].isna() & df['Quantity'].notna() & df['Price Per Unit'].notna()\n",
    "df.loc[mask_ts, 'Total Spent'] = (df.loc[mask_ts, 'Quantity'] * df.loc[mask_ts, 'Price Per Unit']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83b839dfc773975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.076863Z",
     "start_time": "2025-05-19T00:13:33.040304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill Quantity = Total Spent / Price Per Unit\n",
    "mask_q = df['Quantity'].isna() & df['Total Spent'].notna() & df['Price Per Unit'].notna()\n",
    "df.loc[mask_q, 'Quantity'] = (df.loc[mask_q, 'Total Spent'] / df.loc[mask_q, 'Price Per Unit']).round()\n",
    "# Keep nullable integer for missing-friendly arithmetic\n",
    "df['Quantity'] = df['Quantity'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b411c224ffa244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.135735Z",
     "start_time": "2025-05-19T00:13:33.078356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item              401\n",
       "Quantity            0\n",
       "Price Per Unit      0\n",
       "Total Spent         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill Price Per Unit = Total Spent / Quantity\n",
    "mask_ppu = df['Price Per Unit'].isna() & df['Total Spent'].notna() & df['Quantity'].notna()\n",
    "df.loc[mask_ppu, 'Price Per Unit'] = (df.loc[mask_ppu, 'Total Spent'] / df.loc[mask_ppu, 'Quantity']).round(2)\n",
    "\n",
    "# Quick check after the trio fill\n",
    "df[['Item','Quantity','Price Per Unit','Total Spent']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b79c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item filled from price: 401\n"
     ]
    }
   ],
   "source": [
    "# price -> most common item mapping\n",
    "def mode_or_nan(s):\n",
    "    m = s.dropna().mode()\n",
    "    return m.iloc[0] if len(m) else np.nan\n",
    "\n",
    "price_to_item = df.groupby('Price Per Unit')['Item'].apply(mode_or_nan)\n",
    "\n",
    "mask_item = df['Item'].isna() & df['Price Per Unit'].notna()\n",
    "filled_before = int(df['Item'].isna().sum())\n",
    "df.loc[mask_item, 'Item'] = df.loc[mask_item, 'Price Per Unit'].map(price_to_item)\n",
    "filled_after = int(df['Item'].isna().sum())\n",
    "print(f\"Item filled from price: {filled_before - filled_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90bce6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the remaining unidentified items to '<UNKNOWN_ITEM>' since they cannot be determined. \n",
    "df['Item'] = df['Item'].fillna('<UNKNOWN_ITEM>')\n",
    "\n",
    "# Drop rows with missing values in these columns, as only a small number remain.\n",
    "df.dropna(subset=['Quantity','Price Per Unit','Total Spent'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f15bac",
   "metadata": {},
   "source": [
    "‚úÖ Filling complete:\n",
    "- Completed `Quantity`, `Price Per Unit`, and `Total Spent` using mutual derivation (triangle relationship).  \n",
    "- Imputed `Item` values from price‚Üíitem mapping (495 rows filled).  \n",
    "- Assigned **`<UNKNOWN_ITEM>`** to remaining items and dropped unrecoverable rows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c387610172aebcb",
   "metadata": {},
   "source": [
    "### 6.4 Filling `Payment Method`\n",
    "Impute missing values by sampling from the observed distribution to preserve class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0ef9fd1e5c6309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.153483Z",
     "start_time": "2025-05-19T00:13:33.149456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filling:\n",
      " NaN               2691\n",
      "Digital Wallet    1973\n",
      "Credit Card       1952\n",
      "Cash              1928\n",
      "Name: Payment Method, dtype: int64\n",
      "\n",
      "===========================\n",
      "\n",
      "After filling:\n",
      " Digital Wallet    2879\n",
      "Credit Card       2854\n",
      "Cash              2811\n",
      "Name: Payment Method, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Before filling:\\n\", df['Payment Method'].value_counts(dropna=False))\n",
    "\n",
    "# Distribution of observed categories\n",
    "pm_dist = df['Payment Method'].value_counts(normalize=True, dropna=True)\n",
    "\n",
    "# Mask missing\n",
    "mask_pm = df['Payment Method'].isna()\n",
    "\n",
    "# Fill missing by sampling\n",
    "rng = np.random.default_rng(42)\n",
    "df.loc[mask_pm, 'Payment Method'] = rng.choice(\n",
    "    pm_dist.index,\n",
    "    size=mask_pm.sum(),\n",
    "    p=pm_dist.values\n",
    ")\n",
    "\n",
    "print(\"\\n===========================\\n\")\n",
    "print(\"After filling:\\n\", df['Payment Method'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b94e8",
   "metadata": {},
   "source": [
    "‚úÖ Filling `Payment Method` completed:  \n",
    "- 3,178 missing values were filled using random sampling from the observed distribution.  \n",
    "- Final counts are balanced across methods: **Digital Wallet (3,374)**, **Credit Card (3,319)**, **Cash (3,307)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb3257",
   "metadata": {},
   "source": [
    "### 6.5 Filling `Location`\n",
    "Impute missing locations using the empirical distribution so the branch mix is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d11f2909b73f3a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.157793Z",
     "start_time": "2025-05-19T00:13:33.153993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filling:\n",
      " NaN         3373\n",
      "In-Store    2597\n",
      "Takeaway    2574\n",
      "Name: Location, dtype: int64\n",
      "\n",
      "===========================\n",
      "\n",
      "After filling:\n",
      " In-Store    4314\n",
      "Takeaway    4230\n",
      "Name: Location, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Before filling:\\n\", df['Location'].value_counts(dropna=False))\n",
    "\n",
    "# Distribution of observed categories\n",
    "loc_dist = df['Location'].value_counts(normalize=True, dropna=True)\n",
    "\n",
    "# Mask missing\n",
    "mask_loc = df['Location'].isna()\n",
    "\n",
    "# Fill missing by sampling\n",
    "rng = np.random.default_rng(42)\n",
    "df.loc[mask_loc, 'Location'] = rng.choice(\n",
    "    loc_dist.index,\n",
    "    size=mask_loc.sum(),\n",
    "    p=loc_dist.values\n",
    ")\n",
    "\n",
    "print(\"\\n===========================\\n\")\n",
    "print(\"After filling:\\n\", df['Location'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9643e5",
   "metadata": {},
   "source": [
    "‚úÖ Filling `Location` completed:  \n",
    "- 3,961 missing values were imputed using random sampling from the observed distribution.  \n",
    "- Final counts are balanced across categories: **Takeaway (5,015)**, **In-store (4,985)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199ff71",
   "metadata": {},
   "source": [
    "### 6.6 Handling `Transaction Date`\n",
    "Standardize the transaction date column for temporal analysis (`NaT` for invalid values).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab18d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ‚Äî Transaction Date: 385\n"
     ]
    }
   ],
   "source": [
    "# Ensure datetime (in case earlier steps introduced strings)\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')\n",
    "\n",
    "# Keep NaT (Not a Time) for missing dates instead of filling with 'UNKNOWN'\n",
    "# This preserves datetime type consistency for the entire column\n",
    "\n",
    "# Quick check\n",
    "print(\"Missing ‚Äî Transaction Date:\", int(df['Transaction Date'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d68a31",
   "metadata": {},
   "source": [
    "‚úÖ Transaction Date cleaned:\n",
    "- Parsed into proper `datetime` format (invalid entries coerced to NaT).  \n",
    "- Preserves datetime type consistency by keeping missing values as `NaT` instead of mixed types.  \n",
    "- For time-based analysis, filter out missing dates with `df[df['Transaction Date'].notna()]`.  \n",
    "\n",
    "This maintains a clean datetime column that works reliably with pandas datetime operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682899dd8b73b67",
   "metadata": {},
   "source": [
    "## 7: Final Validation & Export\n",
    "\n",
    "*Purpose:* Verify the integrity of the cleaned dataset and prepare it for downstream analysis.\n",
    "\n",
    "- Checked final dataset shape (rows √ó columns).  \n",
    "- Confirmed that no missing values remain.  \n",
    "- Validated standardized data types across all fields.  \n",
    "- Reviewed summary statistics to ensure data consistency.  \n",
    "- Exported the cleansed dataset (`cleansed_cafe_sales.csv`) for profitability and sales behavior analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "244cf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transaction ID uniqueness: PASSED\n",
      "‚úÖ Date range validation: PASSED\n",
      "‚úÖ Numeric range validation: PASSED\n",
      "\n",
      "üéâ ALL QUALITY CHECKS PASSED (if no ‚ùå above)!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Rule 1: Transaction ID uniqueness\n",
    "    assert df['Transaction ID'].is_unique, \"Transaction IDs must be unique\"\n",
    "    print(\"‚úÖ Transaction ID uniqueness: PASSED\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Transaction ID uniqueness: FAILED ‚Äî {e}\")\n",
    "\n",
    "try:\n",
    "    # Rule 2: No future dates\n",
    "    valid_dates = df['Transaction Date'].notna()\n",
    "    future_dates = df.loc[valid_dates, 'Transaction Date'] > pd.Timestamp.now()\n",
    "    assert not future_dates.any(), \"No future dates allowed\"\n",
    "    print(\"‚úÖ Date range validation: PASSED\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Date range validation: FAILED ‚Äî {e}\")\n",
    "\n",
    "try:\n",
    "    # Rule 3: Positive quantities and non-negative prices\n",
    "    assert (df['Quantity'] > 0).all(), \"All quantities must be positive\"\n",
    "    assert (df['Price Per Unit'] >= 0).all(), \"All prices must be non-negative\"\n",
    "    assert (df['Total Spent'] >= 0).all(), \"All totals must be non-negative\"\n",
    "    print(\"‚úÖ Numeric range validation: PASSED\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Numeric range validation: FAILED ‚Äî {e}\")\n",
    "\n",
    "print(\"\\nüéâ ALL QUALITY CHECKS PASSED (if no ‚ùå above)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a7dd2196387dc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.191930Z",
     "start_time": "2025-05-19T00:13:33.190089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final dataset shape (rows √ó columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f84beaa3f3052e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.233089Z",
     "start_time": "2025-05-19T00:13:33.227254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction ID        0\n",
       "Item                  0\n",
       "Quantity              0\n",
       "Price Per Unit        0\n",
       "Total Spent           0\n",
       "Payment Method        0\n",
       "Location              0\n",
       "Transaction Date    385\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify no missing values remain\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f70589027dca0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.274768Z",
     "start_time": "2025-05-19T00:13:33.270612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction ID              object\n",
       "Item                        object\n",
       "Quantity                     Int64\n",
       "Price Per Unit             float64\n",
       "Total Spent                float64\n",
       "Payment Method              object\n",
       "Location                    object\n",
       "Transaction Date    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm data types are standardized\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcb1edfda75d0b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.388035Z",
     "start_time": "2025-05-19T00:13:33.370794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8544</td>\n",
       "      <td>8544</td>\n",
       "      <td>8,544.00</td>\n",
       "      <td>8,544.00</td>\n",
       "      <td>8,544.00</td>\n",
       "      <td>8544</td>\n",
       "      <td>8544</td>\n",
       "      <td>8159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8544</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Juice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-Store</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2879</td>\n",
       "      <td>4314</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-02 03:14:08.486334208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.28</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transaction ID   Item  Quantity  Price Per Unit  Total Spent  \\\n",
       "count            8544   8544  8,544.00        8,544.00     8,544.00   \n",
       "unique           8544      8       NaN             NaN          NaN   \n",
       "top       TXN_1961373  Juice       NaN             NaN          NaN   \n",
       "freq                1   1228       NaN             NaN          NaN   \n",
       "mean              NaN    NaN      3.02            2.95         8.93   \n",
       "min               NaN    NaN      1.00            1.00         1.00   \n",
       "25%               NaN    NaN      2.00            2.00         4.00   \n",
       "50%               NaN    NaN      3.00            3.00         8.00   \n",
       "75%               NaN    NaN      4.00            4.00        12.00   \n",
       "max               NaN    NaN      5.00            5.00        25.00   \n",
       "std               NaN    NaN      1.42            1.28         6.00   \n",
       "\n",
       "        Payment Method  Location               Transaction Date  \n",
       "count             8544      8544                           8159  \n",
       "unique               3         2                            NaN  \n",
       "top     Digital Wallet  In-Store                            NaN  \n",
       "freq              2879      4314                            NaN  \n",
       "mean               NaN       NaN  2023-07-02 03:14:08.486334208  \n",
       "min                NaN       NaN            2023-01-01 00:00:00  \n",
       "25%                NaN       NaN            2023-04-01 00:00:00  \n",
       "50%                NaN       NaN            2023-07-02 00:00:00  \n",
       "75%                NaN       NaN            2023-10-01 00:00:00  \n",
       "max                NaN       NaN            2023-12-31 00:00:00  \n",
       "std                NaN       NaN                            NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review summary statistics for all columns\n",
    "df.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838a75e",
   "metadata": {},
   "source": [
    "‚úÖ The dataset is now fully prepared for further analytical modeling and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11538d3879c75926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:13:33.531171Z",
     "start_time": "2025-05-19T00:13:33.501502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataset for further analysis\n",
    "df.to_csv('cleansed_cafe_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d8ec2",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "The cleaned dataset (`cleansed_cafe_sales.csv`) is now ready for profitability and sales behavior analysis.  \n",
    "Key improvements include:\n",
    "\n",
    "- Removed duplicates and handled missing values.  \n",
    "- Standardized data types and categorical fields.  \n",
    "- Imputed critical fields (`Quantity`, `Price Per Unit`, `Total Spent`, `Item`, `Payment Method`, `Location`, `Transaction Date`).  \n",
    "- Verified final dataset shape, dtypes, and summary statistics.  \n",
    "- Exported the cleansed dataset for downstream use.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
